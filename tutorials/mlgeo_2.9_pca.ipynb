{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "059aa832-245c-4923-b168-e2add62b0222",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Ideally, one would not need to extract or select feature in the input data. However, reducing the dimensionality as a separate pre-processing steps may be advantageous:\n",
    "\n",
    "\n",
    "1. The complexity of the algorithm depends on the number of input dimensions and size of the data.\n",
    "2. If some features are unecessary, not extracting them saves compute time\n",
    "3. Simpler models are more robust on small datasets\n",
    "4. Fewer features lead to a better understanding of the data.\n",
    "5. Visualization is easier in fewer dimensions.\n",
    "\n",
    "\n",
    "Feature *selection* finds the dimensions that explain the data without loss of information and ends with a smaller dimensionality of the input data. A *forward selection* approach starts with one variable that decreases the error the most and add one by one. A *backward selection* starts with all variables and removes them one by one.\n",
    "\n",
    "Feature *extraction* finds a new set of dimension as a combination of the original dimensions. They can be supervised or unsupervised depending on the output information. Examples are **Principal Component Analysis**, **Independent Component Analysis** **Linear Discriminant Analysis**\n",
    "\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "PCA is an unsupervised learning method that finds the mapping from the input to a lower dimemsional space with minimum loss of information. First, we will download again our GPS time series from Oregon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbe9a7-e64a-4d71-bdd9-36503a419179",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PCA idenditfies the axis that accounts for the largest amount of variance in the data. Let's  $\\mathbf{Y} = \\mathbf{y}_1,\\cdots,\\mathbf{y}_n $ be the data ,measured $n$ times over multiple fields of measurements (the length of $\\mathbf{y})$. Each **column** of $\\mathbf{Y}$ is different measurements for single experiments (e.g., single time stamps). Each column of $ \\mathbf{Y} $ is the same measurement repeated over several experiments (or time).  The steps to PCA are:\n",
    "1. center and standardize the data by substracting mean of each row of $\\mathbf{Y} $  (all measurements at a single experiment or time stamp)\n",
    "2. Calculate the covariance matrix of the de-meaned data $\\mathbf{C}  =  \\frac{1}{n-1} \\mathbf{Y}^{\\ast}\\mathbf{Y}$. By definition, the covariance matrix is positive symmetric, and thus can be diagonalized.\n",
    "3. Calculate the Singular Value Decomposition (SVD) of the covariance matrix $\\mathbf{C} $.\n",
    "\n",
    "\n",
    "PCA uses the *(Singular Value Decomposition (SVD)* to decompose the data covariance matrix $\\mathbf{C} $:\n",
    "\n",
    "$\\mathbf{X} = \\mathbf{U} \\Sigma \\mathbf{V}^T ,$\n",
    "\n",
    "where $\\mathbf{V}^T$ contains the eigenvectors, or principal components. The *PCs* are normalized, centered around zero.\n",
    "\n",
    "The *1st principal component* eigenvector that has the highest eigenvalue in the direction that has the highest variance.\n",
    "\n",
    "We will start simple with 10000 data points measured for 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881389fd-3239-4d3a-b073-ae0723a5624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful modules\n",
    "import requests, zipfile, io, gzip, glob, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.linalg as ln\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb512ee3",
   "metadata": {},
   "source": [
    "We will start with fake data, a 2D cloud point of 10,000 measurements over 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create fake data\n",
    "xC = np.array([2, 1])      # Center of data (mean)\n",
    "sig = np.array([2, 0.5])   # Principal axes\n",
    "theta = np.pi/3            # Rotate cloud by pi/3\n",
    "R = np.array([[np.cos(theta), -np.sin(theta)],     # Rotation matrix\n",
    "              [np.sin(theta), np.cos(theta)]])\n",
    "nPoints = 10000            # Create 10,000 points\n",
    "\n",
    "# create the cloud of points by multiplying (np.matmul is also called by @)\n",
    "X = R @ np.diag(sig) @ np.random.randn(2,nPoints) + np.diag(xC) @ np.ones((2,nPoints))\n",
    "# print the shape of teh array\n",
    "print(X.shape)\n",
    "# plot the data\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(X[0,:],X[1,:], '.', color='k')\n",
    "ax1.grid()\n",
    "plt.xlim((-6, 8))\n",
    "plt.ylim((-6,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8c0714",
   "metadata": {},
   "source": [
    "### 1. Substract the mean of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the mean of the data\n",
    "Xavg = np.mean(X,axis=1)                  # Compute mean\n",
    "B = X - np.tile(Xavg,(nPoints,1)).T       # Mean-subtracted data\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679f852",
   "metadata": {},
   "source": [
    "### 2. SVD of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find principal components (SVD): \n",
    "# use the option full_matrices =0 will calculate the covariance of B\n",
    "U, S, VT = np.linalg.svd(B/np.sqrt(nPoints),full_matrices=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be99f580",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.plot(X[0,:],X[1,:], '.', color='k')   # Plot data to overlay PCA\n",
    "ax2.grid()\n",
    "plt.xlim((-6, 8))\n",
    "plt.ylim((-6,8))\n",
    "\n",
    "theta = 2 * np.pi * np.arange(0,1,0.01)\n",
    "\n",
    "# 1-std confidence interval\n",
    "Xstd = U @ np.diag(S) @ np.array([np.cos(theta),np.sin(theta)])\n",
    "\n",
    "ax2.plot(Xavg[0] + Xstd[0,:], Xavg[1] + Xstd[1,:],'-',color='r',linewidth=2)\n",
    "ax2.plot(Xavg[0] + 2*Xstd[0,:], Xavg[1] + 2*Xstd[1,:],'-',color='r',linewidth=2)\n",
    "ax2.plot(Xavg[0] + 3*Xstd[0,:], Xavg[1] + 3*Xstd[1,:],'-',color='r',linewidth=2)\n",
    "\n",
    "# Plot principal components U[:,0]S[0] and U[:,1]S[1]\n",
    "ax2.plot(np.array([Xavg[0], Xavg[0]+U[0,0]*S[0]]),\n",
    "         np.array([Xavg[1], Xavg[1]+U[1,0]*S[0]]),'-',color='cyan',linewidth=3)\n",
    "ax2.plot(np.array([Xavg[0], Xavg[0]+U[0,1]*S[1]]),\n",
    "         np.array([Xavg[1], Xavg[1]+U[1,1]*S[1]]),'-',color='cyan',linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf37758",
   "metadata": {},
   "source": [
    "Now let's work on the 3D component of the surface motions recorded at a GPS data. The 3D components are the displacements in Earth, North, Vertical/Upward. the 3 fields are measured daily over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7e067-1e70-407a-9a65-463d84333559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data and save into a pandas.\n",
    "sta=\"P395\"\n",
    "file_url=\"http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/\"+ sta + \".tenv\"\n",
    "r = requests.get(file_url).text.splitlines()  # download, read text, split lines into a list\n",
    "ue=[];un=[];uv=[];se=[];sn=[];sv=[];date=[];date_year=[];df=[]\n",
    "for iday in r:  # this loops through the days of data\n",
    "    crap=iday.split()\n",
    "    if len(crap)<10:\n",
    "      continue\n",
    "    date.append((crap[1]))\n",
    "    date_year.append(float(crap[2]))\n",
    "    ue.append(float(crap[6])*1000)\n",
    "    un.append(float(crap[7])*1000)\n",
    "    uv.append(float(crap[8])*1000)\n",
    "# Extract the three data dimensions\n",
    "E = np.asarray(ue)# East component\n",
    "N = np.asarray(un)# North component\n",
    "U = np.asarray(uv)# Vertical component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af647d-efb8-460b-b92f-f164a8ad26ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the data\n",
    "fig=plt.figure(figsize=(11,8))\n",
    "ax=fig.add_subplot(projection='3d')\n",
    "ax.scatter(E,N,U);ax.grid(True)\n",
    "ax.set_xlabel('East motions (mm)')\n",
    "ax.set_ylabel('North motions (mm)')\n",
    "ax.set_zlabel('Vertical motions (mm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ddd943-7cdb-4fed-8ced-552b2bfa5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's organize our GPS data in a matrix\n",
    "X = np.vstack([E,N,U])\n",
    "print(U.shape)\n",
    "print(X.shape)\n",
    "nPoints=U.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d9182",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the mean of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ebb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find principal components (SVD): \n",
    "# use the option full_matrices =0 will calculate the covariance of B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigen spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e70fc6",
   "metadata": {},
   "source": [
    "There are 3 components of motion, but the majority of the motion is explained in 1 direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first PC and provide a crude estimate of the azimuthal direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d2a1b-1ae2-48f7-92bc-7ef27fec2769",
   "metadata": {},
   "source": [
    "We will use the ``sklearn`` package. The model ``PCA`` deals with the data transformation (removing mean and standardizing), calculate the covariance, and the SVD.\n",
    "\n",
    "**WARNING:** Numpy and sklean do not organize the data in the same dimension. \n",
    "* Using sklearn, organize the data for each field as a column.\n",
    "* Using numpy, organize the data for each field as a row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00fa19e-43ef-4466-821b-40f3180b3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # this is the SKLEAN model\n",
    "pca=PCA(n_components=3).fit(X.transpose())# retain all 3 components\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c0de2-d376-4c3f-bfb6-7776ad850388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84699a-cbf1-46a6-91ea-21a265f24512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs' explained variance\n",
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a01eb4",
   "metadata": {},
   "source": [
    "Now you see that most of the variance is explained by the first PC. What azimuth is the direction of the first PC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "azimuth=math.degrees(math.atan2(pca.components_[0][0],pca.components_[0][1]))\n",
    "if azimuth <0:azimuth+=360\n",
    "print(\"direction of the plate \",azimuth,\" degrees from North\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e1397-c23d-4b84-9480-f375a3d3e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3 PCs' explained variance ratio: how much of the variance is explained by each component\n",
    "print(pca.explained_variance_ratio_)\n",
    "fig,ax=plt.subplots(2,1,figsize=(11,8))\n",
    "ax[0].plot(pca.explained_variance_ratio_);ax[0].set_xticks(range(0,3))\n",
    "ax[0].set_xlabel('Number of dimensions')\n",
    "ax[0].set_ylabel('Explained variance ')\n",
    "ax[0].set_title('Variance explained with each PC')\n",
    "ax[1].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "ax[1].set_xlabel('Number of dimensions')\n",
    "ax[1].set_ylabel('Explained variance ')\n",
    "ax[1].set_title('Variance explained with cumulated PCs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958de9fc-ef8e-4d87-875a-a4cd9e43d0a9",
   "metadata": {},
   "source": [
    "Now we see that 97% of the variance is explained by the first PC. We can reduce the dimension of our system by only working with a single dimension. Another way to choose the number of dimensions is to select the minimum of PCs that would explain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c09351-6f23-4215-b5a5-6d1724e3884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.argmax(np.cumsum(pca.explained_variance_ratio_)>=0.95) +1\n",
    "print(\"minimum dimension size to explain 95% of the variance \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2b0a52-c599-41ca-84d7-96b68823085f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=d).fit(X)\n",
    "X_pca = pca.transform(X)\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f15e6",
   "metadata": {},
   "source": [
    "Inverse Transform back to the original data, but with only the first PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5024fcd-922c-43a1-a673-84951f8cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the azimuth of the displacement vector\n",
    "X_new = pca.inverse_transform(X_pca)\n",
    "print(X_new.shape)\n",
    "plt.scatter(X[0,:], X[1,:], alpha=1)\n",
    "plt.scatter(X_new[0,:], X_new[1,:], alpha=0.2)\n",
    "plt.legend(['Original data','Reconstructed Data'])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d04dd8-de1c-47e9-afc7-43ee6d85622c",
   "metadata": {},
   "source": [
    "*SVD* can be computationally intensive for larger dimensions. It is recommended to use a **randomized PCA** to approximate the first principal components. Scikit-learn automatically switches to randomized PCA in either the following happens: data size > 500, number of vectors is > 500 and the number of PCs selected is less than 80% of either one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647aa4ae-0c00-47ae-8f15-e85c8e4aa327",
   "metadata": {},
   "source": [
    "## Independent Component Analysis\n",
    "\n",
    "ICA is used to estimate sources given noisy measurements. It is frequently used in Geodesy to isolate contributions from earthquakes and hydrology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e0d7c0-9003-4dff-879a-231802865b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "n_samples = 2000\n",
    "time = np.linspace(0, 8, n_samples)\n",
    "\n",
    "# create 3 source signals\n",
    "s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n",
    "s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n",
    "s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n",
    "\n",
    "\n",
    "S = np.c_[s1, s2, s3]\n",
    "S += 0.2 * np.random.normal(size=S.shape)  # Add noise\n",
    "S /= S.std(axis=0)  # Standardize data\n",
    "\n",
    "print(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59882032-8754-401f-a9f2-3868aa66bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix data\n",
    "# create 3 signals at 3 receivers:\n",
    "A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\n",
    "X = np.dot(S, A.T)  # Generate observations\n",
    "\n",
    "\n",
    "# Compute ICA\n",
    "ica = FastICA(n_components=3)\n",
    "S_ = ica.fit_transform(X)  # Reconstruct signals\n",
    "A_ = ica.mixing_  # Get estimated mixing matrix\n",
    "print(A_,A)\n",
    "# For comparison, compute PCA\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc634c8-f44b-4383-9004-7a4199a8e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,8))\n",
    "models = [X, S, S_, H]\n",
    "names = ['Observations (mixed signal)',\n",
    "         'True Sources',\n",
    "         'ICA recovered signals', \n",
    "         'PCA recovered signals']\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "for ii, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(4, 1, ii)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig, color=color)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee662218-0920-4a44-9bcf-64af0bf6675f",
   "metadata": {},
   "source": [
    "## Other Techniques\n",
    "\n",
    "\n",
    "1. Random Projections\n",
    "https://scikit-learn.org/stable/modules/random_projection.html\n",
    "2. Multidimensional Scaling\n",
    "3. Isomap\n",
    "4. t-Distributed stochastic neighbor embedding\n",
    "5. Linear discriminant analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f625eed87f201675869c1975f26c79747f846dd12cd9c70305bdb23b2c204f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
