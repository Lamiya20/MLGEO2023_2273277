{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665e0d40",
   "metadata": {},
   "source": [
    "# Databases\n",
    "This tutorial will cover the basics of building a database. We will test a relational database, taking the data from a pandas dataframe. We will test a non-relational database using the first database and adding documents to it.\n",
    "\n",
    "The data base we will build is a collection of earthquake events metadata and seismograms together. Both can be two separate relational databases. We will benchmark performance on metadata manipulations.\n",
    "\n",
    "You can find help here: http://swcarpentry.github.io/sql-novice-survey/10-prog/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5224d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c8d32",
   "metadata": {},
   "source": [
    "## 1. Preparing the data\n",
    "We will use the metadata of the seismic stations as a base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439854e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import pickle\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from math import cos, sin, pi, sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb5a58-6167-42cb-b7db-f0d4ac84bd0c",
   "metadata": {},
   "source": [
    "We will use the Northern California Earthquake Data Center stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c3441c-1aaa-48f4-825f-8030035bf77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     station network channel location   rate           start_time  \\\n",
      "0        AAR      NC     EHZ       --  100.0  1984/01/01,00:00:00   \n",
      "1        AAR      NC     EHZ       --  100.0  1987/05/01,00:00:00   \n",
      "2        AAR      NC     SHZ       --   20.0  1994/11/28,00:00:00   \n",
      "3        AAS      NC     EHZ       --  100.0  1984/11/27,18:45:00   \n",
      "4        AAS      NC     EHZ       --  100.0  1987/05/01,00:00:00   \n",
      "...      ...     ...     ...      ...    ...                  ...   \n",
      "6135     WMP      NC     SHN       --   20.0  1995/07/02,12:00:00   \n",
      "6136     WMP      NC     SHZ       --   20.0  1995/03/02,19:00:00   \n",
      "6137     WMP      NC     SHZ       --   20.0  1995/07/02,12:00:00   \n",
      "6138     WMP      NC     SHZ       10   20.0  1995/07/02,12:00:00   \n",
      "6139    WWVB      NC     ATT       --  100.0  1984/01/01,00:00:00   \n",
      "\n",
      "                 end_time  latitude  longitude  elevation  depth   dip  \\\n",
      "0     1987/05/01,00:00:00  39.27594 -121.02696      911.0    0.0 -90.0   \n",
      "1     2006/01/04,19:19:00  39.27594 -121.02696      911.0    0.0 -90.0   \n",
      "2     2006/01/04,19:19:00  39.27594 -121.02696      911.0    0.0 -90.0   \n",
      "3     1987/05/01,00:00:00  38.43014 -121.10959       31.0    0.0 -90.0   \n",
      "4     3000/01/01,00:00:00  38.43014 -121.10959       31.0    0.0 -90.0   \n",
      "...                   ...       ...        ...        ...    ...   ...   \n",
      "6135  2002/05/08,22:30:00  35.64059 -118.78570     1078.0    0.0   0.0   \n",
      "6136  1995/07/02,12:00:00  35.64059 -118.78570     1078.0    0.0 -90.0   \n",
      "6137  2002/05/08,22:30:00  35.64059 -118.78570     1078.0    0.0 -90.0   \n",
      "6138  1999/05/11,23:59:00  35.64059 -118.78570     1078.0    0.0 -90.0   \n",
      "6139  1999/03/20,23:59:00   0.00000    0.00000        1.0    0.0   0.0   \n",
      "\n",
      "      azimuth  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "2         0.0  \n",
      "3         0.0  \n",
      "4         0.0  \n",
      "...       ...  \n",
      "6135      0.0  \n",
      "6136      0.0  \n",
      "6137      0.0  \n",
      "6138      0.0  \n",
      "6139      0.0  \n",
      "\n",
      "[6140 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# get the station information\n",
    "url = 'http://ncedc.org/ftp/pub/doc/NC.info/NC.channel.summary.day'\n",
    "s = requests.get(url).content\n",
    "data = pd.read_csv(io.StringIO(s.decode('utf-8')), header=None, skiprows=2, sep='\\s+', usecols=list(range(0, 13)))\n",
    "data.columns = ['station', 'network', 'channel', 'location', 'rate', 'start_time', 'end_time', 'latitude', 'longitude', 'elevation', 'depth', 'dip', 'azimuth']\n",
    "data.to_csv('ncedc_stations.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a331d8c-e7f8-4adb-8318-8b351cd692d1",
   "metadata": {},
   "source": [
    "We will download earthquake waveforms from Ariane's earthquake catalog of repeating earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4855d9-9617-498a-8741-70b506d091d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will look at one earthquake that occured in the Mendicino area, M5.2 2020/3/18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7352b9d3-4baf-4f9b-984c-bc4ca000ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stations that recorded that earthquake. Create maks to find stations within 100 km radius from the earthquake and that recorded at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c822ec",
   "metadata": {},
   "source": [
    "## 2. Relational database: SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0b4615-84f9-459f-8119-dff1dc3f8ff2",
   "metadata": {},
   "source": [
    "This is an example on how to dump a pandas dataframe into a SQL database. But honestly, i can't seem to figure out how to query it afterwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e61243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<sqlalchemy.engine.base.Connection object at 0x12195e340>\n",
      "<sqlalchemy.engine.cursor.LegacyCursorResult object at 0x121bf2f70>\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///ncedc_stations_sql.db',echo=False)\n",
    "db_sql = engine.connect()\n",
    "data_sql=data.to_sql('data_db_sql',db_sql,index=False,\\\n",
    "               if_exists='append')\n",
    "data_db_sql=engine.execute(\"SELECT * FROM data_db_sql\")\n",
    "\n",
    "# I think that is how things work, but i can't seem to query the database..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7ec3c",
   "metadata": {},
   "source": [
    "## 3. Nonrelational document database: MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124a5bba-59c4-4572-b701-119b494e3044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /Users/marinedenolle/opt/anaconda3/envs/uwdsgeo/lib/python3.9/site-packages (3.11.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23256356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('607d11268d18e448b9cb870e'), 'index': 0, 'station': 'AAR', 'network': 'NC', 'channel': 'EHZ', 'location': '--', 'rate': 100.0, 'start_time': '1984/01/01,00:00:00', 'end_time': '1987/05/01,00:00:00', 'latitude': 39.27594, 'longitude': -121.02696, 'elevation': 911.0, 'depth': 0.0, 'dip': -90.0, 'azimuth': 0.0}\n",
      "   \n",
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'NCEDC'), 'stations')\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "mongo_client = MongoClient('localhost', 27017)# this will create a local db (default is cloud service)\n",
    "\n",
    "mydb=mongo_client['NCEDC']\n",
    "\n",
    "doc = mydb['stations']\n",
    "#data.reset_index(inplace=True)\n",
    "\n",
    "data_dict = data.to_dict(\"records\")\n",
    "# Insert collection\n",
    "\n",
    "doc.insert_many(data_dict)\n",
    "print(mydb.stations.find_one())\n",
    "print(\"   \")\n",
    "print(doc)\n",
    "\n",
    "data.to_json('ncedc_stations_mongo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b17c0-ef01-44c3-9a83-42e07f2c19e4",
   "metadata": {},
   "source": [
    "Now the advantage of non-relational databases and document stores are that we can also add other files/data types into the database. We will add the earthquake catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "674dfcbe-c9fe-4028-9382-123dfacbddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('607d8ca5b270e79aecc45e18'), 'year': 2007, 'month': 10, 'day': 13, 'hour': 9, 'minute': 55, 'second': 3.9, 'cc': 0.10682520309805875, 'nchannel': 15, 'date': datetime.datetime(2007, 10, 13, 9, 55, 3, 900000)}\n",
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'NCEDC'), 'stations')\n",
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'NCEDC'), 'earthquakes')\n"
     ]
    }
   ],
   "source": [
    "namefile = 'catalog_2007_2009.pkl'\n",
    "tbegin = datetime(2007, 9, 25, 0, 0, 0)\n",
    "tend = datetime(2009, 5, 14, 0, 0, 0)\n",
    "dt = 10.0\n",
    "thresh1 = 1.4\n",
    "thresh2 = 1.9\n",
    "df1 = pickle.load(open(namefile, 'rb'))\n",
    "df1 = df1[['year', 'month', 'day', 'hour', 'minute', 'second', 'cc', 'nchannel']]\n",
    "df1 = df1.astype({'year': int, 'month': int, 'day': int, 'hour': int, 'minute': int, 'second': float, 'cc': float, 'nchannel': int})\n",
    "date = pd.to_datetime(df1.drop(columns=['cc', 'nchannel']))\n",
    "df1['date'] = date\n",
    "df1 = df1[(df1['date'] >= tbegin) & (df1['date'] <= tend)]\n",
    "df1_filter = df1.loc[df1['cc'] * df1['nchannel'] >= thresh1]\n",
    "data_dict = df1_filter.to_dict(\"records\")\n",
    "\n",
    "\n",
    "# doc = mydb['stations']\n",
    "doc2 = mydb['earthquakes']\n",
    "doc2.insert_many(data_dict)\n",
    "\n",
    "print(mydb.earthquakes.find_one())\n",
    "print(doc)\n",
    "print(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ab110",
   "metadata": {},
   "source": [
    "## 4. Benchmarking exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ca4f62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.29 µs\n",
      "Pandas sorted\n",
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 4.05 µs\n",
      "Mongo sorted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# from sqlalchemy import desc, select\n",
    "\n",
    "# sorting by station nam\n",
    "%time\n",
    "data.sort_values(\"station\") # sort the pandas\n",
    "print('Pandas sorted')\n",
    "\n",
    "%time\n",
    "mydb[\"stations\"].find().sort(\"station\") # sort the mongoDB\n",
    "print('Mongo sorted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4a48b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 11 µs, total: 13 µs\n",
      "Wall time: 14.1 µs\n",
      "Pandas sorted\n",
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.81 µs\n",
      "Mongo sorted\n"
     ]
    }
   ],
   "source": [
    "# sorting by date of the earthquakes\n",
    "%time\n",
    "df1_filter.sort_values(\"date\") # sort the pandas\n",
    "print('Pandas sorted')\n",
    "\n",
    "%time\n",
    "mydb[\"earthquakes\"].find().sort(\"date\") # sort the mongoDB\n",
    "print('Mongo sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d983af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# group by\n",
    "%time\n",
    "data.groupby('station').station.count()\n",
    "print('Pandas group by stations')\n",
    "\n",
    "%time\n",
    "mydb[\"stations\"].aggregate([\\\n",
    "         {\"$unwind\": \"$station\"},\\\n",
    "         {\"$group\": {\"_id\": \"$station\", \"count\": {\"$sum\": 1}}},\\\n",
    "  ])\n",
    "\n",
    "# ([{ \"$group\": { \"_id\": \"station\": { },\n",
    "#         \"count\": { \"$sum\": 1 }\n",
    "#     }}\n",
    "# ]) # sort the mongoDB\n",
    "print('Mongo group by station')\n",
    "\n",
    "\n",
    "# Good luck with SQL\n",
    "# %time\n",
    "# print(db_sql)\n",
    "# print(data_sql)\n",
    "# print(data_db_sql)\n",
    "# crap=connection.cursor()\n",
    "# s=select([crap.c.stations]).order_by(desc(data_db_sql.c.station))\n",
    "# stm=connection.execute(s)\n",
    "# print('SQL sorted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
