{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c78dd-00a9-42d0-b0d8-b0509fcc625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86cfe65-4d1e-4d26-8ff4-7c4729932f20",
   "metadata": {},
   "source": [
    "# Deep Neural Networks and their training\n",
    "\n",
    "A neural network is considered \"deep\" if it has \"many\" layers (more than 5).\n",
    "\n",
    "Shiny new methods like DNN are reported with amazing performance and accuracy. Be aware that only the best performance get put in papers, the majority of the time is spent with failures on model design and training.\n",
    "\n",
    "A major issue with DNN stems from its dependence on the gradient of the cost functions. Because the gradients uses the chain rule with multiplication of errors in each layer, they can vanish or explode if the variance increases with each layer. It happens that the logistic function (sigmoid) having a mean of 0.5, and saturating quickly to either 0 or 1 (gradients of the sigmoid tends to zero quickly).\n",
    "\n",
    "## Layer initialization\n",
    "Glorot and He (2010) proposes an initialization that helps control the variance of the outputs in each layer. The number of inputs (fan<sub>in</sub> ) and the numbers of neurons (fan<sub>out</sub>). fan<sub>avg</sub> = (fan<sub>in</sub> +fan<sub>out</sub> )/2.\n",
    "By default, Keras uses a Glorot initialization with a uniform distribution. But we can change the distribution to normal.\n",
    "\n",
    "| initialization | activation functions      | distribution width ($\\sigma^2$)|\n",
    "| -------------- | --------------------------|------------------------------- |\n",
    "| Glorot         | None,tanh,logistic,softmax|1/fan<sub>avg</sub>             |\n",
    "| He             | Relu & variantes          | 1/fan<sub>in</sub>             |\n",
    "| LeCun          | SELU                      | 1/fan<sub>in</sub>             |\n",
    "\n",
    "One can initialize the layers using these distributions as kernels: \n",
    "\n",
    "``keras.layers.Dense(300,activation=\"relu\",kernel_initializer=\"he_normal\")``\n",
    "\n",
    "or choose a custom layer with a He inialisation of uniform distribution based out of fan<sub>ave</sub> instead of fan<sub>in</sub>:\n",
    "\n",
    "\n",
    "``my_custom_init = keras.initializers.VarianceScaling(scale=2.,model='fan_avg',distribution='uniform')``\n",
    "\n",
    "`` keras.layers.Dense(300,activation=\"relu\",kernel_initializer=my_custom_init)``\n",
    "\n",
    "More information here: https://keras.io/api/layers/initializers/\n",
    "\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "Sigmoids are not the best choice for DNN due to their saturation. ReLU is not great either because it outputs 0, so that if the weights become always negative, the neuron will \"die\".\n",
    "\n",
    "\n",
    "* **ReLu**  : $ReLU(z)=max(0,z)$.\n",
    "* **LeakyReLU** : $ReLU_\\alpha(z) = max(\\alpha z,z) $, $\\alpha<0.02$. Outperforms ReLU. Hard to tune the hyperparameter $\\alpha$. To use Leaky ReLU, you need to add a layer:\n",
    "\n",
    "`` keras.layers.Dense(10,kernel_initializer=\"he_normal\");keras.layers.LeakyReLU(alpha=0.2)``.\n",
    "\n",
    "* **Randomized Leaky ReLU - RReLU** : $ReLU_\\alpha(z) = max(\\alpha z,z) $.  hyperparameter $\\alpha$ is randomized during training, fixed during testing. Actas are a regularizer and reduces the risk of overfitting.\n",
    "* **Parametric leaky ReLU (PReLu)**: $\\alpha$ is learned during training. Outperforms ReLU for large data sets, runs the risk to overfit the smaller data sets.\n",
    "* **Exponential Linear Unit ELU** :$ELU_\\alpha(z) = \\alpha (\\exp(z)-1) (z<0); z (z>=0)$ . Outperforms all of the variants of ReLUs: training time reduced, never kills the neurons, smooth everywhere if $\\alpha=1$ that helps speed up GD. Drawbacks are slowness in computing, but compensated by faster convergence.\n",
    "* **Scaled ELU - SELU** (Klambauer et al, 2017): a scaled variant of ELU. It self-normalizes the network because the output of each layer will tend to preserve a zero mean. However, it requires: input features must be standardized (mean 0, std=1), initialization of weights must be LeCun normal (kernel_initializer=\"lecun_normal\"), network must be sequential (no slip connection, nor RNN, no wide nets), layers must be dense.\n",
    "\n",
    "In general, the preferred choice should be:\n",
    "\n",
    "**SELU (if dense and sequential) > ELU > Leaky ReLU (and variants) > ReLU > tanh > logistic.**\n",
    "\n",
    "ReLU is the most implemented, and most packages have optimizers that best perform on ReLU, so a safe choice is ReLU. More information here: https://keras.io/api/layers/activation_layers/\n",
    "\n",
    "# Batch Normalization\n",
    "\n",
    "The BN layer standardizes the inputs (shift and normalize the outputs of the layers). It adds parameters and complexity to the network. The benefits are:\n",
    "* it reduces vanishing/exploding gradients (one can now use saturating activation functions),\n",
    "* networks are less sensitive to the weight initialization,\n",
    "* one can use larger learning rates,\n",
    "* they act as a regularizer (reducing the need for regularization and dropout). \n",
    "\n",
    "Tips on implementation:\n",
    "* Whether the BN layer should be before or after the activation function seems to depend on the task, so one has to experiment with each data set.\n",
    "* To add the BN layer before the activation function, you must remove the activation function from the hidden layers and add them separately \n",
    "* Because BN removes the bias term from the previous layers, you can remove the bias term from the previous layers ``use_bias=False``.\n",
    "* the hyperparameter ``momentum``  that is used to update the averages <1 but close to one 0.999s for large data sets and small mini batches.\n",
    "* BN layers by default acts on one axis, use the argument ``axis=[1,2]`` for 2D input batch (or flatten your 2D input batch to 1D).\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50d14360-a04b-4fff-995f-2cd6f206a5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Dense(300,kernel_initializer=\"he_normal\",use_bias=False), # single dense layer, downsampling from input layer to this year from 784 points to 300.\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"relu\"),\n",
    "keras.layers.Dense(100,kernel_initializer=\"he_normal\",use_bias=False), # 100 neurons\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.Activation(\"relu\"),\n",
    "keras.layers.Dense(10,activation=\"softmax\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620ae3d-47c3-44dc-b3bf-3f7b3a4b2425",
   "metadata": {},
   "source": [
    "If Batch Normalization is tricky to use (e.g. in RNNs), one can simply clip the gradients so that they do not explode. You can clip the value of the gradients by a tunable/hyperparameter value ``clipvalue=1.0`` or by the L2 norm of the gradients ``clipnorm=1.0`` (it preserves the orientation of the gradient. Try both and see what works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8b551-dfec-4adb-b198-6f8c79f419b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.SGD(lr=alpha,clipvalue=1.0) # stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd0067-4658-4047-9cea-ddd2817fd818",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "\n",
    "Essential steps to many geoscience problems. Use previously trained models, add on your feature, and re-train on your data to improve generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "645a9210-9c25-4daa-9fb5-bf180e15a870",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_first_NN_model.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1]) # use all of the network exept the last year.\n",
    "model_B_on_A.add(keras.layers.Dense(10,activation=\"softmax\")) # this is your new model: here i reset the output layer\n",
    "model_A_clone = keras.models.clone_model(model_A) # CLONE MODELS TO AVOID OVERWRITING!\n",
    "model_A_clone.set_weights(model_A.get_weights()) # COPY THE WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83b81e-ed47-4ae7-8571-8265db079241",
   "metadata": {},
   "source": [
    "Training model_B_on_A will modify  model_A, thus one has to clone model_A to keep an unmodified version.\n",
    "\n",
    "When you retrain but modified and added layers, there is an inbalance of weights or information: the old model is already optimized, the new layers are random. One can freeze the pre-trained layers to force the training on the newly added layers during a few epochs. One can freeze layers by switching the status of the parameters \"trainable\" from \"True\" to \"False\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e88086b-2e3d-4af4-a8a2-93dedaacb0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=False\n",
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.SGD(lr=alpha) # stochastic gradient descent\n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d43e14-1140-4056-b216-8c97ab7d9462",
   "metadata": {},
   "source": [
    "Train on a few epochs, then unfreeze the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baa6a09d-5127-4157-893e-295217a27236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fashion MNIST data. it's boring, but it works!\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "class_names = [\"tshirt\",\"trousers\",\"pullover\",\"dress\",\"coat\",\"sandal\",\"shirt\",\"sneaker\",\"bag\",\"boot\"]\n",
    "X_val,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_val,y_train = y_train_full[:5000],y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b056e7f6-40bf-49e6-9225-fd2e975896ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1719/1719 [==============================] - 1s 776us/step - loss: 1.0702 - accuracy: 0.6753 - val_loss: 0.4437 - val_accuracy: 0.8680\n",
      "Epoch 2/4\n",
      "1719/1719 [==============================] - 1s 701us/step - loss: 0.4125 - accuracy: 0.8774 - val_loss: 0.3829 - val_accuracy: 0.8752\n",
      "Epoch 3/4\n",
      "1719/1719 [==============================] - 1s 702us/step - loss: 0.3532 - accuracy: 0.8887 - val_loss: 0.3593 - val_accuracy: 0.8820\n",
      "Epoch 4/4\n",
      "1719/1719 [==============================] - 1s 707us/step - loss: 0.3313 - accuracy: 0.8915 - val_loss: 0.3478 - val_accuracy: 0.8834\n",
      "Epoch 1/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3160 - accuracy: 0.8933 - val_loss: 0.3562 - val_accuracy: 0.8790\n",
      "Epoch 2/16\n",
      "1719/1719 [==============================] - 2s 984us/step - loss: 0.3046 - accuracy: 0.8950 - val_loss: 0.3480 - val_accuracy: 0.8750\n",
      "Epoch 3/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2935 - accuracy: 0.8964 - val_loss: 0.3449 - val_accuracy: 0.8768\n",
      "Epoch 4/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2942 - accuracy: 0.8971 - val_loss: 0.3305 - val_accuracy: 0.8850\n",
      "Epoch 5/16\n",
      "1719/1719 [==============================] - 2s 999us/step - loss: 0.2821 - accuracy: 0.9019 - val_loss: 0.3375 - val_accuracy: 0.8818\n",
      "Epoch 6/16\n",
      "1719/1719 [==============================] - 2s 991us/step - loss: 0.2744 - accuracy: 0.9022 - val_loss: 0.3290 - val_accuracy: 0.8822\n",
      "Epoch 7/16\n",
      "1719/1719 [==============================] - 2s 991us/step - loss: 0.2762 - accuracy: 0.9015 - val_loss: 0.3198 - val_accuracy: 0.8896\n",
      "Epoch 8/16\n",
      "1719/1719 [==============================] - 2s 993us/step - loss: 0.2742 - accuracy: 0.9034 - val_loss: 0.3173 - val_accuracy: 0.8874\n",
      "Epoch 9/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2717 - accuracy: 0.9057 - val_loss: 0.3160 - val_accuracy: 0.8898\n",
      "Epoch 10/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2701 - accuracy: 0.9036 - val_loss: 0.3240 - val_accuracy: 0.8870\n",
      "Epoch 11/16\n",
      "1719/1719 [==============================] - 2s 999us/step - loss: 0.2616 - accuracy: 0.9081 - val_loss: 0.3234 - val_accuracy: 0.8848\n",
      "Epoch 12/16\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2639 - accuracy: 0.9081 - val_loss: 0.3094 - val_accuracy: 0.8904\n",
      "Epoch 13/16\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.2579 - accuracy: 0.9085 - val_loss: 0.3151 - val_accuracy: 0.8878\n",
      "Epoch 14/16\n",
      "1719/1719 [==============================] - 2s 993us/step - loss: 0.2596 - accuracy: 0.9069 - val_loss: 0.3315 - val_accuracy: 0.8852\n",
      "Epoch 15/16\n",
      "1719/1719 [==============================] - 2s 999us/step - loss: 0.2518 - accuracy: 0.9104 - val_loss: 0.3241 - val_accuracy: 0.8856\n",
      "Epoch 16/16\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.2509 - accuracy: 0.9105 - val_loss: 0.3090 - val_accuracy: 0.8878\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train,y_train,epochs=4,validation_data=(X_val,y_val))\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable=True\n",
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.SGD(lr=alpha) # stochastic gradient descent\n",
    "model_B_on_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")\n",
    "history = model_B_on_A.fit(X_train,y_train,epochs=16,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1611fd4-34e3-4eaa-a4a2-fc5a86ee8261",
   "metadata": {},
   "source": [
    "Fully connected DNNs tend to not generalize well. Transfer learning works best with deep CNNs.\n",
    "\n",
    "# Optimizers\n",
    "\n",
    "Training is super slow. You can speed up training with the following strategies:\n",
    "* good initialization\n",
    "* good activation function\n",
    "* use Batch Normalization\n",
    "* use Lasso Regression for sparse models.\n",
    "\n",
    "Optimizer comparisons:\n",
    "* **SGD**. Slow convergence speed but high convergence quality. Default in keras is ``keras.optimizers.SGD()``. Default is constant learning rate, but one can impose an evolution of the learning rate as training goes. ``keras.optimizers.SGD(lr=0.01,decay=1e-4)`` implements are power scheduling of the learning rate $\\eta$ ,$\\eta(t)=\\eta_0/(1+t/s)^c$, where $c$ is the power of the decay ($1/s$). There are other functions that can be implemented, check out the Keras information: https://keras.io/api/callbacks/learning_rate_scheduler/\n",
    "* **momentum optimization**. The weights are updated by a momentum vector that carries the sum of the gradients, it's a lot faster though bounces around a bit at the end.  ``keras.optimizers.SGD(lr=0.001,momentum=0.9)``. Usually 0.9 works great, but it should be tuned as a hyperparmeter\n",
    "* **Nesterov Accelerated Gradient**. It's momentum + the gradient of the momentum. NAG is much faster than momentum GD. ``keras.optimizers.SGD(lr=0.001,momentum=0.9,nesterov=True)``.\n",
    "* **AdaGrad**. It scales the gradient vector to the steepest dimensions. It has an *adaptive learning rate* because the learning rate depends on the cost function gradient itself. One issue with neural networks is that the learning rate becomes too small, so do NOT use it for DNN.\n",
    "* **Adam Optimization (best choice)**: adaptive moment estimation. It uses both momentum (beta_1 hyper parameter) and sum of gradients (beta_2) as scaling mechanisms for updating the weights. Good default values are : ``keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)``.\n",
    "\n",
    "# Avoiding Overfitting for training DNNs\n",
    "\n",
    "## Regularizations\n",
    "Use l_1 and l_2 regularizations to constrain the connection weights. Use l_1 to force a sparse model. To implement a regularization on each layer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d02849d6-e668-4683-9ac1-c3440373f9aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x14c9504c0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\",\\\n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01)) # this is your new model: here i reset the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06531843-7484-428b-85a6-618c71ed3727",
   "metadata": {},
   "source": [
    "Because stacking layers means calling the keras functions with many hyperparameters, typing each layer by hand is prone to typos and errors. It is recommended to write functions to wrap these calls. In Python, the function ``functools.partial()`` can perform just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1e254b3-833e-49ed-ae97-38f9a4efb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "RegDense = partial(keras.layers.Dense,\n",
    "                   activation=\"relu\",\n",
    "                   kernel_initializer=\"he_normal\",\n",
    "                   kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "RegDense(300),\n",
    "RegDense(100),\n",
    "RegDense(10,activation=\"softmax\",kernel_initializer=\"glorot_uniform\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba294d5-ef6a-4f34-a260-a8dd5d75d7c6",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Most popular regularization technique for DNN. It is muting a portion of the neurons during the training, forcing the gradients to pass through neurons that could be otherwise ignored.\n",
    "\n",
    "![Multi Layer Perceptron](dropout2.svg)\n",
    "\n",
    "Monte Carlo Dropout serves the other purpose ot estimating an ensemble of testing. We make N predections over the test set, setting ``training=True`` to ensure that the dropout layer is still active and stack the predictions. Because the dropout is active, all the predictions woll be different. Averaging over multiple predictions with dropout gives us a MC estimate that is more reliable than the result of a single prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b65d18-d756-4178-b17c-9fe67ed4fd12",
   "metadata": {},
   "source": [
    "### marine will update this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe14064b-f2a8-4a82-862b-ee618f1ae20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential( [\n",
    "keras.layers.Flatten(input_shape=[28,28]), # reshape the 2D matrix into a 1D vector, without modifying the values\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(300,activation=\"relu\"), # \n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(100,activation=\"relu\"), # 100 neurons\n",
    "keras.layers.Dropout(rate=0.2),\n",
    "keras.layers.Dense(10,activation=\"softmax\") ]) # output layer, 10 neurons since there are 10 classes.\n",
    "alpha = 0.005# learning rate lr\n",
    "optimizer = keras.optimizers.Adam(lr=0.001,beta_1=0.9,beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aeff1899-9f87-48b8-995c-770c3a4c41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c492c05-ac05-469f-83c3-d29c3aaeabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.7481 - accuracy: 0.7296 - val_loss: 0.4242 - val_accuracy: 0.8434\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8319 - val_loss: 0.3722 - val_accuracy: 0.8610\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4157 - accuracy: 0.8474 - val_loss: 0.3588 - val_accuracy: 0.8706\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3963 - accuracy: 0.8539 - val_loss: 0.3288 - val_accuracy: 0.8854\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3800 - accuracy: 0.8615 - val_loss: 0.3318 - val_accuracy: 0.8762\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3689 - accuracy: 0.8650 - val_loss: 0.3313 - val_accuracy: 0.8788\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3601 - accuracy: 0.8677 - val_loss: 0.3121 - val_accuracy: 0.8816\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3479 - accuracy: 0.8702 - val_loss: 0.3055 - val_accuracy: 0.8886\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3423 - accuracy: 0.8726 - val_loss: 0.3108 - val_accuracy: 0.8858\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3317 - accuracy: 0.8760 - val_loss: 0.3116 - val_accuracy: 0.8848\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3332 - accuracy: 0.8748 - val_loss: 0.3008 - val_accuracy: 0.8904\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3217 - accuracy: 0.8814 - val_loss: 0.2936 - val_accuracy: 0.8908\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3166 - accuracy: 0.8822 - val_loss: 0.3121 - val_accuracy: 0.8830\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.3194 - accuracy: 0.8809 - val_loss: 0.3162 - val_accuracy: 0.8844\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3133 - accuracy: 0.8821 - val_loss: 0.2948 - val_accuracy: 0.8918\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3038 - accuracy: 0.8846 - val_loss: 0.2902 - val_accuracy: 0.8982\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3012 - accuracy: 0.8873 - val_loss: 0.2903 - val_accuracy: 0.8962\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3012 - accuracy: 0.8864 - val_loss: 0.2887 - val_accuracy: 0.8918\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2968 - accuracy: 0.8870 - val_loss: 0.2880 - val_accuracy: 0.8938\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2895 - accuracy: 0.8906 - val_loss: 0.2932 - val_accuracy: 0.8968\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2920 - accuracy: 0.8900 - val_loss: 0.2866 - val_accuracy: 0.8978\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2906 - accuracy: 0.8903 - val_loss: 0.2868 - val_accuracy: 0.8968\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2887 - accuracy: 0.8918 - val_loss: 0.3021 - val_accuracy: 0.8900\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2874 - accuracy: 0.8926 - val_loss: 0.2877 - val_accuracy: 0.8956\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2883 - accuracy: 0.8910 - val_loss: 0.2957 - val_accuracy: 0.8906\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2829 - accuracy: 0.8929 - val_loss: 0.2849 - val_accuracy: 0.8990\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2841 - accuracy: 0.8918 - val_loss: 0.3010 - val_accuracy: 0.8932\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2809 - accuracy: 0.8940 - val_loss: 0.2851 - val_accuracy: 0.9022\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2883 - accuracy: 0.8899 - val_loss: 0.2925 - val_accuracy: 0.9024\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2791 - accuracy: 0.8947 - val_loss: 0.2780 - val_accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "checkpoints_cb = keras.callbacks.ModelCheckpoint(\"my_first_NN_model.h5\",save_best_only=True)\n",
    "# with the argument save_best_only, the checkpoint saved will be one that of best performance according to the performance metrics chosen.\n",
    "history = model.fit(X_train,y_train,epochs=30,validation_data=(X_val,y_val),callbacks=[checkpoints_cb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
