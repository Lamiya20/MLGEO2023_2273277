{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 Hyperparameter Tuning\n",
    "\n",
    "The choice of model parameters that is optimal for the specific problem can be found automatically by searching the model parameter space. The type of algorithm is fixed in this instance.\n",
    "\n",
    "There are built-in toolkits to perform the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wget\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits,fetch_openml\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# explore data type\n",
    "data,y = digits[\"data\"].copy(),digits[\"target\"].copy()\n",
    "print(type(data[0][:]),type(y[0]))\n",
    "# note that we do not modify the raw data that is stored on the digits dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 15.0\n",
      "There are 1797 data samples\n"
     ]
    }
   ],
   "source": [
    "print(min(data[0]),max(data[0]))\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)# fit the model for data normalization\n",
    "newdata = scaler.transform(data) # transform the data. watch that data was converted to a numpy array\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "print(f\"There are {data.shape[0]} data samples\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "clf = SVC(gamma=0.001) # model design\n",
    "clf.fit(X_train, y_train) # learn\n",
    "svc_prediction = clf.predict(X_test) # predict on test\n",
    "print(\"SVC Accuracy:\", metrics.accuracy_score(y_true=y_test ,y_pred=svc_prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what are the parameters we are trying to optimize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 0.001,\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as ``SVC()``);\n",
    "\n",
    "* a parameter space;\n",
    "\n",
    "* a method for searching or sampling candidates;\n",
    "\n",
    "* a cross-validation scheme; and\n",
    "\n",
    "* a loss function.\n",
    "\n",
    "\n",
    "## 1. Grid Search cross validation. \n",
    "Performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is ``GridSearchCV``. More details [here](!https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "  {'C': [1, 5,10,50, 100,500, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1,5, 10,50, 100,500, 1000], 'gamma': [0.01,0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm will search for all combinations of parameters, which can be from the model algorithms or the choice of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(clf, param_grid, cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train) # learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Search Cross Validation.\n",
    "It performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is ``GridSearchCV``. More details [here](!https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "The advantage is that it can be used for a wide hyperparameter space and limit to ``n_iter`` number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVC(gamma=0.001), n_iter=100,\n",
       "                   param_distributions=[{'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x14da1faf0>,\n",
       "                                         'kernel': ['linear']},\n",
       "                                        {'C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x14da1ff40>,\n",
       "                                         'gamma': <scipy.stats._distn_infrastructure.rv_frozen object at 0x14c8986a0>,\n",
       "                                         'kernel': ['rbf']}],\n",
       "                   random_state=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform,norm ,loguniform\n",
    "\n",
    " \n",
    "distributions= [ {'C': uniform(loc=1, scale=1000), 'kernel': ['linear']},\n",
    "  {'C': uniform(loc=1, scale=1000), 'gamma': loguniform(1e-4,1e-2), 'kernel': ['rbf']}]\n",
    "clf2 = RandomizedSearchCV(clf, distributions, random_state=0,cv=5,n_iter=100)\n",
    "clf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(search.best_params_)\n",
    "print(clf2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble learning\n",
    "\n",
    "The group of models performs better than individual models. For example, Random Forests perform better than the individual Decision Trees that constitute the Forest.\n",
    "\n",
    "\n",
    "### 3.1 Voting Classifier\n",
    "\n",
    "Aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "![Voting Classifer](votingclassifier.png)\n",
    "\n",
    "From \"Hands on Machine Learning With Sci-kit Learn, Keras, and Tensorflow\" (Gueron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo_sk/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9388888888888889"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "svc_clf = SVC(gamma=0.001) # model design\n",
    "log_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [ ('lr',log_clf) , ('rf',rf_clf),('svc',svc_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train) # learn\n",
    "y_pred=voting_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with individual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo_sk/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.9055555555555556\n",
      "RandomForestClassifier 0.9222222222222223\n",
      "SVC 0.9583333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo_sk/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.9472222222222222\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf,rf_clf,svc_clf,voting_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f625eed87f201675869c1975f26c79747f846dd12cd9c70305bdb23b2c204f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
